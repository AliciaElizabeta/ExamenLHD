A hash map organizes data so you can quickly look up values for a given key.\hypertarget{md_src_main_java_com_thealgorithms_datastructures_hashmap__readme_autotoc_md3}{}\doxysection{Strengths\+:}\label{md_src_main_java_com_thealgorithms_datastructures_hashmap__readme_autotoc_md3}

\begin{DoxyItemize}
\item {\bfseries{Fast lookups}}\+: Lookups take O(1) time on average.
\item {\bfseries{Flexible keys}}\+: Most data types can be used for keys, as long as they\textquotesingle{}re hashable.
\end{DoxyItemize}\hypertarget{md_src_main_java_com_thealgorithms_datastructures_hashmap__readme_autotoc_md4}{}\doxysection{Weaknesses\+:}\label{md_src_main_java_com_thealgorithms_datastructures_hashmap__readme_autotoc_md4}

\begin{DoxyItemize}
\item {\bfseries{Slow worst-\/case}}\+: Lookups take O(n) time in the worst case.
\item {\bfseries{Unordered}}\+: Keys aren\textquotesingle{}t stored in a special order. If you\textquotesingle{}re looking for the smallest key, the largest key, or all the keys in a range, you\textquotesingle{}ll need to look through every key to find it.
\item {\bfseries{Single-\/directional lookups}}\+: While you can look up the value for a given key in O(1) time, looking up the keys for a given value requires looping through the whole dataset—\+O(n) time.
\item {\bfseries{Not cache-\/friendly}}\+: Many hash table implementations use linked lists, which don\textquotesingle{}t put data next to each other in memory.
\end{DoxyItemize}\hypertarget{md_src_main_java_com_thealgorithms_datastructures_hashmap__readme_autotoc_md5}{}\doxysection{Time Complexity}\label{md_src_main_java_com_thealgorithms_datastructures_hashmap__readme_autotoc_md5}
\tabulinesep=1mm
\begin{longtabu}spread 0pt [c]{*{3}{|X[-1]}|}
\hline
\PBS\centering \cellcolor{\tableheadbgcolor}\textbf{ }&\PBS\centering \cellcolor{\tableheadbgcolor}\textbf{ AVERAGE   }&\PBS\centering \cellcolor{\tableheadbgcolor}\textbf{ WORST    }\\\cline{1-3}
\endfirsthead
\hline
\endfoot
\hline
\PBS\centering \cellcolor{\tableheadbgcolor}\textbf{ }&\PBS\centering \cellcolor{\tableheadbgcolor}\textbf{ AVERAGE   }&\PBS\centering \cellcolor{\tableheadbgcolor}\textbf{ WORST    }\\\cline{1-3}
\endhead
Space   &O(n)   &O(n)    \\\cline{1-3}
Insert   &O(1)   &O(n)    \\\cline{1-3}
Lookup   &O(1)   &O(n)    \\\cline{1-3}
Delete   &O(1)   &O(n)   \\\cline{1-3}
\end{longtabu}
\hypertarget{md_src_main_java_com_thealgorithms_datastructures_hashmap__readme_autotoc_md6}{}\doxysection{Internal Structure of Hash\+Map}\label{md_src_main_java_com_thealgorithms_datastructures_hashmap__readme_autotoc_md6}
Internally Hash\+Map contains an array of Node and a node is represented as a class that contains 4 fields\+:
\begin{DoxyItemize}
\item int hash
\item K key
\item V value
\item Node next
\end{DoxyItemize}

It can be seen that the node is containing a reference to its own object. So it’s a linked list.\hypertarget{md_src_main_java_com_thealgorithms_datastructures_hashmap__readme_autotoc_md7}{}\doxysection{Performance of Hash\+Map}\label{md_src_main_java_com_thealgorithms_datastructures_hashmap__readme_autotoc_md7}
Performance of Hash\+Map depends on 2 parameters which are named as follows\+:
\begin{DoxyItemize}
\item Initial Capacity
\item Load Factor
\end{DoxyItemize}

{\bfseries{Initial Capacity}}\+: It is the capacity of Hash\+Map at the time of its creation (It is the number of buckets a Hash\+Map can hold when the Hash\+Map is instantiated). In java, it is 2$^\wedge$4=16 initially, meaning it can hold 16 key-\/value pairs.

{\bfseries{Load Factor}}\+: It is the percent value of the capacity after which the capacity of Hashmap is to be increased (It is the percentage fill of buckets after which Rehashing takes place). In java, it is 0.\+75f by default, meaning the rehashing takes place after filling 75\% of the capacity.

{\bfseries{Threshold}}\+: It is the product of Load Factor and Initial Capacity. In java, by default, it is (16 $\ast$ 0.\+75 = 12). That is, Rehashing takes place after inserting 12 key-\/value pairs into the Hash\+Map.

{\bfseries{Rehashing}} \+: It is the process of doubling the capacity of the Hash\+Map after it reaches its Threshold. In java, Hash\+Map continues to rehash(by default) in the following sequence – 2$^\wedge$4, 2$^\wedge$5, 2$^\wedge$6, 2$^\wedge$7, …. so on.

If the initial capacity is kept higher then rehashing will never be done. But by keeping it higher increases the time complexity of iteration. So it should be chosen very cleverly to increase performance. The expected number of values should be taken into account to set the initial capacity. The most generally preferred load factor value is 0.\+75 which provides a good deal between time and space costs. The load factor’s value varies between 0 and 1.


\begin{DoxyCode}{0}
\DoxyCodeLine{Note: From Java 8 onward, Java has started using Self Balancing BST instead of a linked list for chaining. }
\DoxyCodeLine{The advantage of self-\/balancing bst is, we get the worst case (when every key maps to the same slot) search time is O(Log n). }

\end{DoxyCode}


Java has two hash table classes\+: Hash\+Table and Hash\+Map. In general, you should use a Hash\+Map.

While both classes use keys to look up values, there are some important differences, including\+:


\begin{DoxyItemize}
\item A Hash\+Table doesn\textquotesingle{}t allow null keys or values; a Hash\+Map does.
\item A Hash\+Table is synchronized to prevent multiple threads from accessing it at once; a Hash\+Map isn\textquotesingle{}t.
\end{DoxyItemize}\hypertarget{md_src_main_java_com_thealgorithms_datastructures_hashmap__readme_autotoc_md8}{}\doxysection{When Hash Map operations cost O(n) time?}\label{md_src_main_java_com_thealgorithms_datastructures_hashmap__readme_autotoc_md8}
{\bfseries{Hash collisions}}\+: If all our keys caused hash collisions, we\textquotesingle{}d be at risk of having to walk through all of our values for a single lookup (in the example above, we\textquotesingle{}d have one big linked list). This is unlikely, but it could happen. That\textquotesingle{}s the worst case.

{\bfseries{Dynamic array resizing}}\+: Suppose we keep adding more items to our hash map. As the number of keys and values in our hash map exceeds the number of indices in the underlying array, hash collisions become inevitable. To mitigate this, we could expand our underlying array whenever things start to get crowded. That requires allocating a larger array and rehashing all of our existing keys to figure out their new position—\+O(n) time. 